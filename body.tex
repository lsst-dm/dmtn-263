\def\DB{ObsLocTAP database }

\section{Introduction}
Vera C. Rubin Observatory is required to publish the observing schedule nightly. Following our VO first policy, we  choose ObsLocTap \citep{2021ivoa.spec.0724S} \footnote{\url{https://www.ivoa.net/documents/ObsLocTAP/}} as the protocol.

This short document discusses some design details.

\section{Requirements}

\citeds{LSE-30} (OSS) requires publication at least two hours ahead of observing.

\citeds{LSE-61} (DMSR) has
DMS-REQ-0353
Specification: A service shall be provided to publish to the community the next visit location and the predicted visit schedule provided by the OCS. This service shall consist of both a web page for human inspection and a web API to allow automated tools to respond promptly.


In \citeds{DMTN-199} the agencies insist we publish a schedule 24 hours ahead of observing.
The lack of fidelity of such a schedule is acknowledged by the agencies.

Though an official LCR should be made it is clear we must provide the schedule 24 hours in advance.


\section{Design}


There are three parts to this:


\begin{enumerate}
\item The forecast  which comes from the scheduler.
\item The history of which observations were made which we will need to construct.
\item Publishing compliant with the IVOA ObsLocTap  spec \citep{2021ivoa.spec.0724S}.
\end{enumerate}

\subsection{Scheduler forecast} \label{sec:forecast}
The scheduler puts out an event record with a number of observations in it\footnote{\url{https://ts-scheduler.lsst.io/developer-guide/developer-guide.html\#operation-modes}}.
Currently the requirement is 2 hours and the system is built around that kind of window.
The observing conditions and schedule are felt to be relatively stable on that time frame.
But let us assume there will be  a record for 24 hours.\footnote{
At 24 hours before we would, at the start of the observing night, need to produce the schedule  \emph{for the next night}.
}
This shorter record may be produced as frequently as every observation is made with an updated schedule depending on weather etc.
Basically when the scheduler has an update a new event is published to SAL.

These events are available at the USDF in the EFD and the last may be found with this influx query:
\begin{lstlisting}
SELECT * FROM "efd"."autogen"."lsst.sal.Scheduler.logevent_predictedSchedule
            where numberOfTargets > 0 ORDER BY DESC LIMIT 1
\end{lstlisting}

The current structure of this record is a bunch of columnar arrays representing a time and pointing with the
values : \texttt{mjd, ra, dec, rotSkyPos, nexp}.
It is understood more values may follow such as field/tile and filter.
The potential mapping of these values to the ObsLocTap \texttt{Characterization} is given in \tabref{tab:schema}.
We will however pick this up from the Kafka stream directly.


{\bf It seems a separate 24 hour schedule produced each night is preferable to the scheduler team. This makes the forecast easy. There would then also be several shorter \texttt{predictedSchedule} events to deal with. }

We are required to give 24 hours notice but that is not yet in the  baseline requirements.
Any consumers of this 24 hour schedule, as stated in \citeds{DMTN-199}, must acknowledge the lower probability of the observation being carried out compared to the 2 hour updates made during the night.

\subsubsection{Handling the schedule}
We will need to look at each schedule event record in as it arrives.
The best way to do this is to listen tot he Kafka stream which populates the EFD.

An initial prototype could simply publish this to a web page or JSON file.
This may be a useful tool in any case\footnote{A start has been made in \url{https://github.com/lsst-dm/obsloctap}}.


For ObsLocTap a separate record must be created for each pointing in a database lets call this the \DB in this document.
ObsLocTap refers to such an entry as a \texttt{ Characterization}.
Putting this in a Postgres database would seem feasible and allow a fairly standard TAP implementation to be deployed atop.
Postgres is already available at USDF.
IVOA ObsLocTap considers a table called \texttt{obsplan} and the schema is fixed in the spec (reproduced in \appref{sec:obsplan}).
The schema shall be  defined in the schema repository
\footnote{ SDM Schema repo \url{https://github.com/lsst/sdm_schemas/tree/main/yml}}
 allowing us to generate the SQL from the description in the repo.

Based on the required schema of ObsLocTap (reproduced in \appref{sec:obsplan} our schema and mapping to the fields shown in \secref{sec:forecast} is provided in \tabref{tab:schema}.

\input{schema}


So let us assume we shall have a Postgres table with the IVOA fields as in \tabref{tab:schema}.
As each new event record is seen the new pointings not already in the database must be added.
If there is a 24 hour schedule some of the new pointings will replace existing ones, the existing entries should be marked \texttt{unscheduled}.
The simple approach we would follow  for new \texttt{predictedSchedule} events is to \texttt {unschedule} any existing \texttt {Characterization} in the time span of the new event.

\subsection{History of observations}
Next we need to record which observations were made to fill the \texttt{ObsDataSet}.
There is no ID in the scheduler forecast so there is no explicit matching of observation to forecast.
There is no known plan to add such an ID.
For every observation made we shall have to do either:
\begin{enumerate}
\item If there is a record in the \DB for this pointing mark it as done\\ (set \texttt{execution\_status=Performed}).  We may want to duplicate more than the time to this table but the exposure ID allows one to get any info we needed from the butler registry.
Initially at least we will stick with the schema in the ObsLocTap spec.
\item It there is no record create a record with  all the forecast field blank.\footnote{It is not clear the ObsLocTap requires this.} If there is a record around this time but not this pointing/filter mark that one  \texttt{execution\_status=Unscheduled}
\end{enumerate}

This table would now overlap the exposure log but it is not obvious we should combine these as they serve slightly different purposes.
It is further noted by the scheduler team that very few of the scheduled observations would be made as planned. The

\subsubsection{Was the  observation made ?}\label{sec:made}
There is probably no clear cut answer to this given that there is no ID associated with the forecast.
If we assume users of ObsLocTap cared about co-observing then their object could be anywhere in the
$9 deg^2$ area around the pointing.
For a first stab lets say the observation was made if all the following are true :

\begin{enumerate}
\item The filter used matched the filter in the forecast.
This would seem to fairly important but perhaps it is not - I am unsure of the probability of
doing the scheduled observation with the wrong filter. It seems unlikely.
\item The \texttt {ra, dec} of the forecast matches (the commanded position should match ).
\item The shutter close time of the observation was within 30s of the mjd of the forecast.
We are unlikely to be exactly on time so one exposure error seems reasonable.
\end{enumerate}

This is somewhat arbitrary but at least provides a codeable starting point for discussion.

\subsubsection{Finding the observations}
The butler at the USDF is ingesting the images from the summit within a minute or so.
The butler registry can then provide the metadata needed for  \secref{sec:made}
Again whether this is polling or call back should be discussed.
Some assistance from  a butler guru will also be needed to work out an efficient query.
The prompt processing is trigger at USDF would provide the correct event for doing this for example since it fires on each new image.

\subsection{Deployment}
It seems the USDF is the correct place for a small process to run which populates the \DB and exposes the obsplan table for the TAP service (\secref{sec:tap}).
This should be deployed with Phalanx\footnote{\url{https://phalanx.lsst.io}}.

\subsection{Publishing} \label{sec:tap}
Assuming we constructed and populate the obsplan table in the previous sections publishing means having a TAP service for that table.
We already use CADC TAP on Qserv and there is a version for Postgres so this step may be fairly straight forward.

Once the TAP service is available then it must be registered.
This is describe in section 5 of the spec \citep{2021ivoa.spec.0724S}.
